#!/usr/bin/env python3
"""
ç½®ä¿¡åº¦è®¡ç®—æ”¹è¿›ç¤ºä¾‹
æ¼”ç¤ºæ›´åŠ æ™ºèƒ½çš„ç½®ä¿¡åº¦è®¡ç®—æ–¹æ³•
"""

import math
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from enum import Enum


class ConfidenceLevel(Enum):
    """ç½®ä¿¡åº¦ç­‰çº§"""
    VERY_HIGH = "very_high"    # 0.9-1.0
    HIGH = "high"              # 0.8-0.9
    MEDIUM = "medium"          # 0.6-0.8
    LOW = "low"                # 0.4-0.6
    VERY_LOW = "very_low"      # 0.0-0.4


@dataclass
class ConfidenceMetrics:
    """ç½®ä¿¡åº¦è®¡ç®—æŒ‡æ ‡"""
    retrieval_quality: float   # æ£€ç´¢è´¨é‡ 0-1
    semantic_relevance: float  # è¯­ä¹‰ç›¸å…³æ€§ 0-1
    content_completeness: float # å†…å®¹å®Œæ•´æ€§ 0-1
    user_context_match: float  # ç”¨æˆ·éœ€æ±‚åŒ¹é…åº¦ 0-1

    def overall_score(self, weights: Dict[str, float] = None) -> float:
        """è®¡ç®—ç»¼åˆç½®ä¿¡åº¦åˆ†æ•°"""
        if weights is None:
            weights = {
                'retrieval_quality': 0.3,
                'semantic_relevance': 0.3,
                'content_completeness': 0.2,
                'user_context_match': 0.2
            }

        score = (
            self.retrieval_quality * weights['retrieval_quality'] +
            self.semantic_relevance * weights['semantic_relevance'] +
            self.content_completeness * weights['content_completeness'] +
            self.user_context_match * weights['user_context_match']
        )

        return min(max(score, 0.0), 1.0)


class EnhancedConfidenceCalculator:
    """å¢å¼ºç‰ˆç½®ä¿¡åº¦è®¡ç®—å™¨"""

    def __init__(self):
        self.user_feedback_history = []  # ç”¨æˆ·åé¦ˆå†å²
        self.model_performance_cache = {}  # æ¨¡å‹æ€§èƒ½ç¼“å­˜

    def calculate_retrieval_quality(self, retrieval_results: Dict) -> float:
        """è®¡ç®—æ£€ç´¢è´¨é‡åˆ†æ•°"""
        materials = retrieval_results.get('materials', [])
        essays = retrieval_results.get('essays', [])

        # åŸºç¡€æ•°é‡åˆ†æ•°
        material_score = min(len(materials) / 3, 1.0) * 0.5
        essay_score = min(len(essays) / 2, 1.0) * 0.5

        # æ£€ç´¢ç›¸å…³æ€§åˆ†æ•°ï¼ˆå¦‚æœæœ‰åˆ†æ•°ä¿¡æ¯ï¼‰
        relevance_scores = []
        for material in materials:
            if hasattr(material, 'score'):
                relevance_scores.append(material.score)

        for essay in essays:
            if hasattr(essay, 'score'):
                relevance_scores.append(essay.score)

        # å¹³å‡ç›¸å…³æ€§åˆ†æ•°
        avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.5

        # ç»¼åˆè¯„åˆ†ï¼šæ•°é‡åˆ†æ•° + ç›¸å…³æ€§åˆ†æ•°
        quality_score = (material_score + essay_score) * 0.6 + avg_relevance * 0.4

        return min(quality_score, 1.0)

    def calculate_semantic_relevance(self, query: str, generated_content: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸å…³æ€§ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…å¯ç”¨æ›´å¤æ‚çš„NLPæ¨¡å‹ï¼‰"""
        # è¿™é‡Œæ˜¯ç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥ä½¿ç”¨å¥å‘é‡ç›¸ä¼¼åº¦
        query_words = set(query.lower().split())
        content_words = set(generated_content.lower().split())

        if not query_words:
            return 0.5

        # è®¡ç®—è¯æ±‡é‡å åº¦
        overlap = len(query_words.intersection(content_words))
        relevance = overlap / len(query_words)

        # åº”ç”¨sigmoidå‡½æ•°è¿›è¡Œå¹³æ»‘
        return 1 / (1 + math.exp(-5 * (relevance - 0.5)))

    def calculate_content_completeness(self, guidance) -> float:
        """è®¡ç®—å†…å®¹å®Œæ•´æ€§"""
        scores = []

        # ä¸»é¢˜åˆ†æå®Œæ•´æ€§
        if hasattr(guidance, 'theme_analysis') and guidance.theme_analysis:
            analysis_score = min(len(guidance.theme_analysis) / 100, 1.0)
            scores.append(analysis_score)

        # ç»“æ„å»ºè®®å®Œæ•´æ€§
        if hasattr(guidance, 'structure_suggestion') and guidance.structure_suggestion:
            structure_score = min(len(guidance.structure_suggestion) / 3, 1.0)
            scores.append(structure_score)

        # å†™ä½œæŠ€å·§å®Œæ•´æ€§
        if hasattr(guidance, 'writing_tips') and guidance.writing_tips:
            tips_score = min(len(guidance.writing_tips) / 4, 1.0)
            scores.append(tips_score)

        # å…³é”®è¦ç‚¹å®Œæ•´æ€§
        if hasattr(guidance, 'key_points') and guidance.key_points:
            points_score = min(len(guidance.key_points) / 4, 1.0)
            scores.append(points_score)

        return sum(scores) / len(scores) if scores else 0.0

    def calculate_user_context_match(self, prompt, user_requirements: str, guidance) -> float:
        """è®¡ç®—ç”¨æˆ·éœ€æ±‚åŒ¹é…åº¦"""
        match_score = 0.5  # åŸºç¡€åˆ†æ•°

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ä½œæ–‡ç±»å‹è¦æ±‚
        if hasattr(prompt, 'essay_type') and hasattr(guidance, 'theme_analysis'):
            if prompt.essay_type.value in guidance.theme_analysis.lower():
                match_score += 0.1

        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³éš¾åº¦ç­‰çº§è¦æ±‚
        if hasattr(prompt, 'difficulty_level') and hasattr(guidance, 'writing_tips'):
            difficulty_keywords = {
                'elementary': ['ç®€å•', 'åŸºç¡€', 'å°å­¦'],
                'middle': ['é€‚ä¸­', 'åˆä¸­', 'ä¸­ç­‰'],
                'high': ['é«˜çº§', 'é«˜ä¸­', 'å¤æ‚']
            }

            level = prompt.difficulty_level.value
            keywords = difficulty_keywords.get(level, [])

            for keyword in keywords:
                if any(keyword in tip for tip in guidance.writing_tips):
                    match_score += 0.1
                    break

        # æ£€æŸ¥ç”¨æˆ·ç‰¹æ®Šè¦æ±‚
        if user_requirements:
            req_words = set(user_requirements.lower().split())
            guidance_text = ' '.join([
                guidance.theme_analysis or '',
                ' '.join(guidance.structure_suggestion or []),
                ' '.join(guidance.writing_tips or [])
            ]).lower()

            matched_reqs = sum(1 for word in req_words if word in guidance_text)
            if req_words:
                match_score += 0.3 * (matched_reqs / len(req_words))

        return min(match_score, 1.0)

    def calculate_enhanced_confidence(
        self,
        prompt,
        retrieval_results: Dict,
        guidance,
        user_requirements: str = ""
    ) -> Tuple[float, ConfidenceLevel, Dict[str, float]]:
        """è®¡ç®—å¢å¼ºç‰ˆç½®ä¿¡åº¦"""

        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        retrieval_quality = self.calculate_retrieval_quality(retrieval_results)

        # æ„å»ºæŸ¥è¯¢æ–‡æœ¬
        query_text = f"{prompt.title} {prompt.description or ''} {' '.join(prompt.keywords or [])}"
        guidance_text = f"{guidance.theme_analysis or ''} {' '.join(guidance.structure_suggestion or [])}"
        semantic_relevance = self.calculate_semantic_relevance(query_text, guidance_text)

        content_completeness = self.calculate_content_completeness(guidance)
        user_context_match = self.calculate_user_context_match(prompt, user_requirements, guidance)

        # åˆ›å»ºæŒ‡æ ‡å¯¹è±¡
        metrics = ConfidenceMetrics(
            retrieval_quality=retrieval_quality,
            semantic_relevance=semantic_relevance,
            content_completeness=content_completeness,
            user_context_match=user_context_match
        )

        # æ ¹æ®åœºæ™¯åŠ¨æ€è°ƒæ•´æƒé‡
        weights = self._get_dynamic_weights(prompt)

        # è®¡ç®—æœ€ç»ˆåˆ†æ•°
        final_score = metrics.overall_score(weights)

        # ç¡®å®šç½®ä¿¡åº¦ç­‰çº§
        confidence_level = self._get_confidence_level(final_score)

        # è¿”å›è¯¦ç»†ä¿¡æ¯
        details = {
            'retrieval_quality': retrieval_quality,
            'semantic_relevance': semantic_relevance,
            'content_completeness': content_completeness,
            'user_context_match': user_context_match,
            'weights_used': weights
        }

        return final_score, confidence_level, details

    def _get_dynamic_weights(self, prompt) -> Dict[str, float]:
        """æ ¹æ®åœºæ™¯åŠ¨æ€è°ƒæ•´æƒé‡"""
        # é»˜è®¤æƒé‡
        weights = {
            'retrieval_quality': 0.3,
            'semantic_relevance': 0.3,
            'content_completeness': 0.2,
            'user_context_match': 0.2
        }

        # æ ¹æ®ä½œæ–‡ç±»å‹è°ƒæ•´æƒé‡
        if hasattr(prompt, 'essay_type'):
            if prompt.essay_type.value == 'argumentative':
                # è®®è®ºæ–‡æ›´ä¾èµ–æ£€ç´¢è´¨é‡
                weights['retrieval_quality'] = 0.4
                weights['semantic_relevance'] = 0.25
                weights['content_completeness'] = 0.2
                weights['user_context_match'] = 0.15
            elif prompt.essay_type.value == 'narrative':
                # è®°å™æ–‡æ›´æ³¨é‡å†…å®¹å®Œæ•´æ€§
                weights['retrieval_quality'] = 0.2
                weights['semantic_relevance'] = 0.25
                weights['content_completeness'] = 0.35
                weights['user_context_match'] = 0.2

        # æ ¹æ®éš¾åº¦ç­‰çº§è°ƒæ•´
        if hasattr(prompt, 'difficulty_level'):
            if prompt.difficulty_level.value == 'elementary':
                # å°å­¦é˜¶æ®µæ›´æ³¨é‡ç”¨æˆ·éœ€æ±‚åŒ¹é…
                weights['user_context_match'] += 0.1
                weights['semantic_relevance'] -= 0.05
                weights['content_completeness'] -= 0.05

        return weights

    def _get_confidence_level(self, score: float) -> ConfidenceLevel:
        """æ ¹æ®åˆ†æ•°ç¡®å®šç½®ä¿¡åº¦ç­‰çº§"""
        if score >= 0.9:
            return ConfidenceLevel.VERY_HIGH
        elif score >= 0.8:
            return ConfidenceLevel.HIGH
        elif score >= 0.6:
            return ConfidenceLevel.MEDIUM
        elif score >= 0.4:
            return ConfidenceLevel.LOW
        else:
            return ConfidenceLevel.VERY_LOW

    def get_confidence_message(self, level: ConfidenceLevel, score: float) -> str:
        """æ ¹æ®ç½®ä¿¡åº¦ç­‰çº§ç”Ÿæˆç”¨æˆ·å‹å¥½çš„æ¶ˆæ¯"""
        messages = {
            ConfidenceLevel.VERY_HIGH: f"ğŸŒŸ ç³»ç»Ÿç”Ÿæˆäº†é«˜è´¨é‡çš„å†™ä½œæŒ‡å¯¼ï¼ˆç½®ä¿¡åº¦ï¼š{score:.1%}ï¼‰ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨",
            ConfidenceLevel.HIGH: f"âœ… ç³»ç»Ÿç”Ÿæˆäº†ä¼˜è´¨çš„å†™ä½œå»ºè®®ï¼ˆç½®ä¿¡åº¦ï¼š{score:.1%}ï¼‰ï¼Œè´¨é‡æœ‰ä¿éšœ",
            ConfidenceLevel.MEDIUM: f"ğŸ“ ç³»ç»Ÿç”Ÿæˆäº†ä¸é”™çš„å†™ä½œæŒ‡å¯¼ï¼ˆç½®ä¿¡åº¦ï¼š{score:.1%}ï¼‰ï¼Œå»ºè®®ç»“åˆå…¶ä»–èµ„æ–™å‚è€ƒ",
            ConfidenceLevel.LOW: f"âš ï¸ å½“å‰æŒ‡å¯¼è´¨é‡ä¸€èˆ¬ï¼ˆç½®ä¿¡åº¦ï¼š{score:.1%}ï¼‰ï¼Œå»ºè®®è¡¥å……æ›´å¤šä¿¡æ¯æˆ–é‡æ–°æè¿°éœ€æ±‚",
            ConfidenceLevel.VERY_LOW: f"ğŸ”„ å½“å‰æŒ‡å¯¼è´¨é‡æœ‰é™ï¼ˆç½®ä¿¡åº¦ï¼š{score:.1%}ï¼‰ï¼Œå»ºè®®é‡æ–°å°è¯•æˆ–å¯»æ±‚å…¶ä»–å¸®åŠ©"
        }

        return messages.get(level, f"ç½®ä¿¡åº¦ï¼š{score:.1%}")


# ä½¿ç”¨ç¤ºä¾‹
def demonstrate_enhanced_confidence():
    """æ¼”ç¤ºå¢å¼ºç‰ˆç½®ä¿¡åº¦è®¡ç®—"""
    calculator = EnhancedConfidenceCalculator()

    # æ¨¡æ‹Ÿæ•°æ®
    class MockPrompt:
        def __init__(self):
            self.title = "æˆ‘çš„å®¶ä¹¡"
            self.description = "æå†™å®¶ä¹¡çš„ç¾ä¸½é£æ™¯"
            self.essay_type = type('EssayType', (), {'value': 'descriptive'})()
            self.difficulty_level = type('DifficultyLevel', (), {'value': 'elementary'})()
            self.keywords = ['å®¶ä¹¡', 'é£æ™¯', 'ç¾ä¸½']

    class MockGuidance:
        def __init__(self):
            self.theme_analysis = "è¿™æ˜¯ä¸€ç¯‡æå†™å®¶ä¹¡é£æ™¯çš„å°å­¦ä½œæ–‡ï¼Œé‡ç‚¹åœ¨äºé€šè¿‡å…·ä½“çš„æ™¯ç‰©æå†™å±•ç°å®¶ä¹¡çš„ç¾ä¸½ç‰¹è‰²ï¼Œè¡¨è¾¾å¯¹å®¶ä¹¡çš„å–œçˆ±ä¹‹æƒ…ã€‚"
            self.structure_suggestion = [
                "å¼€å¤´ï¼šç‚¹æ˜å®¶ä¹¡ä½ç½®ï¼Œè¡¨è¾¾å–œçˆ±",
                "ä¸»ä½“ï¼šåˆ†æ®µæå†™ä¸åŒæ™¯ç‰©",
                "ç»“å°¾ï¼šæ€»ç»“å‡åï¼ŒæŠ’å‘æƒ…æ„Ÿ"
            ]
            self.writing_tips = [
                "ä½¿ç”¨æ„Ÿå®˜æå†™",
                "è¿ç”¨ä¿®è¾æ‰‹æ³•",
                "èå…¥çœŸæƒ…å®æ„Ÿ",
                "è¯­è¨€ç®€æ´ç”ŸåŠ¨"
            ]
            self.key_points = [
                "é€‰æ‹©å…·ä½“æ™¯ç‰©",
                "æ³¨æ„æå†™é¡ºåº",
                "è¡¨è¾¾çœŸå®æƒ…æ„Ÿ",
                "æ§åˆ¶æ–‡ç« ç¯‡å¹…"
            ]

    prompt = MockPrompt()
    guidance = MockGuidance()
    retrieval_results = {
        'materials': [type('Material', (), {'score': 0.8})()] * 2,
        'essays': [type('Essay', (), {'score': 0.9})()] * 1
    }

    # è®¡ç®—ç½®ä¿¡åº¦
    score, level, details = calculator.calculate_enhanced_confidence(
        prompt, retrieval_results, guidance, "é‡ç‚¹æŒ‡å¯¼æ™¯ç‰©æå†™æŠ€å·§"
    )

    print(f"=== å¢å¼ºç‰ˆç½®ä¿¡åº¦è®¡ç®—ç»“æœ ===")
    print(f"æœ€ç»ˆå¾—åˆ†: {score:.3f}")
    print(f"ç½®ä¿¡åº¦ç­‰çº§: {level.value}")
    print(f"ç”¨æˆ·æ¶ˆæ¯: {calculator.get_confidence_message(level, score)}")
    print(f"\nè¯¦ç»†åˆ†è§£:")
    for metric, value in details.items():
        if metric != 'weights_used':
            print(f"  {metric}: {value:.3f}")

    print(f"\nä½¿ç”¨çš„æƒé‡:")
    for weight_name, weight_value in details['weights_used'].items():
        print(f"  {weight_name}: {weight_value:.3f}")


if __name__ == "__main__":
    demonstrate_enhanced_confidence()
