"""
æ··åˆæ£€ç´¢å™¨
ç»“åˆå…³é”®è¯æ£€ç´¢å’Œå‘é‡æ£€ç´¢
"""
from typing import List, Dict, Any, Tuple, Optional
from loguru import logger

from ..core.models import EssayPrompt, WritingMaterial, SampleEssay, DocumentChunk
from ..core.utils import extract_keywords, calculate_similarity
from ..knowledge.base import BaseKnowledgeBase
from .vector_store import VectorStore


class HybridRetriever:
    """æ··åˆæ£€ç´¢å™¨"""

    def __init__(self, knowledge_base: BaseKnowledgeBase, vector_store: VectorStore):
        self.knowledge_base = knowledge_base
        self.vector_store = vector_store

        # æ£€ç´¢æƒé‡é…ç½®
        self.keyword_weight = 0.3
        self.semantic_weight = 0.7
        self.material_weight = 0.6
        self.essay_weight = 0.4

    def retrieve_for_prompt(self, prompt: EssayPrompt, top_k: int = 10) -> Dict[str, Any]:
        """ä¸ºä½œæ–‡é¢˜ç›®æ£€ç´¢ç›¸å…³å†…å®¹"""
        try:
            logger.info("ğŸ” å¼€å§‹æ··åˆæ£€ç´¢ (å…³é”®è¯ + è¯­ä¹‰æ£€ç´¢)")

            # æ„å»ºæŸ¥è¯¢æ–‡æœ¬
            query_text = self._build_query_text(prompt)
            logger.info(f"ğŸ” æ„å»ºçš„æŸ¥è¯¢æ–‡æœ¬: {query_text}")

            # å…³é”®è¯æ£€ç´¢
            logger.info("ğŸ“ æ‰§è¡Œå…³é”®è¯æ£€ç´¢...")
            keyword_results = self._keyword_retrieval(query_text, prompt)
            logger.info(f"ğŸ“ å…³é”®è¯æ£€ç´¢ç»“æœ: {len(keyword_results)} é¡¹")

            # å‘é‡æ£€ç´¢
            logger.info("ğŸ§  æ‰§è¡Œè¯­ä¹‰æ£€ç´¢...")
            semantic_results = self._semantic_retrieval(query_text, top_k)
            logger.info(f"ğŸ§  è¯­ä¹‰æ£€ç´¢ç»“æœ: {len(semantic_results)} é¡¹")

            # åˆå¹¶å’Œé‡æ’åºç»“æœ
            logger.info("ğŸ”„ åˆå¹¶å’Œé‡æ’åºæ£€ç´¢ç»“æœ...")
            combined_results = self._combine_results(
                keyword_results, semantic_results, top_k
            )
            logger.info(f"ğŸ”„ åˆå¹¶åç»“æœ: {len(combined_results)} é¡¹")

            # åˆ†ç¦»ç´ æå’ŒèŒƒæ–‡
            materials, essays = self._separate_content_types(combined_results)

            # æˆªå–ç»“æœå¹¶æ·»åŠ åˆ†æ•°
            final_materials = materials[:max(1, int(top_k * self.material_weight))]
            final_essays = essays[:max(1, int(top_k * self.essay_weight))]

            # è®°å½•è¯¦ç»†çš„æ£€ç´¢ç»“æœ
            logger.info("ğŸ“Š æœ€ç»ˆæ£€ç´¢ç»“æœè¯¦æƒ…:")
            logger.info(f"  - ç´ æ: {len(final_materials)} ä¸ª")
            if final_materials:
                for i, (material, score) in enumerate(final_materials[:3], 1):
                    logger.info(f"    {i}. ã€{material.category}ã€‘{material.title} (å¾—åˆ†: {score:.3f})")
                    # ä¸ºç´ ææ·»åŠ åˆ†æ•°å±æ€§ä»¥ä¾¿åç»­ä½¿ç”¨
                    material.score = score

            logger.info(f"  - èŒƒæ–‡: {len(final_essays)} ç¯‡")
            if final_essays:
                for i, (essay, score) in enumerate(final_essays[:3], 1):
                    logger.info(f"    {i}. ã€{essay.essay_type}ã€‘{essay.title} (å¾—åˆ†: {score:.3f})")
                    # ä¸ºèŒƒæ–‡æ·»åŠ åˆ†æ•°å±æ€§ä»¥ä¾¿åç»­ä½¿ç”¨
                    essay.score = score

            # æå–å†…å®¹å¯¹è±¡ï¼ˆä¸åŒ…å«åˆ†æ•°ï¼‰
            materials_only = [item[0] for item in final_materials]
            essays_only = [item[0] for item in final_essays]

            return {
                "materials": materials_only,
                "essays": essays_only,
                "query_text": query_text,
                "keyword_results_count": len(keyword_results),
                "semantic_results_count": len(semantic_results),
                "total_results": len(combined_results)
            }
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            return {
                "materials": [],
                "essays": [],
                "query_text": "",
                "keyword_results_count": 0,
                "semantic_results_count": 0,
                "total_results": 0
            }

    def _build_query_text(self, prompt: EssayPrompt) -> str:
        """æ„å»ºæŸ¥è¯¢æ–‡æœ¬"""
        query_parts = [prompt.title]

        if prompt.description:
            query_parts.append(prompt.description)

        if prompt.keywords:
            query_parts.extend(prompt.keywords)

        if prompt.requirements:
            query_parts.extend(prompt.requirements)

        return " ".join(query_parts)

    def _keyword_retrieval(self, query: str, prompt: EssayPrompt) -> List[Tuple[Any, float, str]]:
        """å…³é”®è¯æ£€ç´¢"""
        try:
            results = []

            # æœç´¢ç´ æ
            materials = self.knowledge_base.search_materials(query, top_k=10)
            for material in materials:
                # è®¡ç®—åŒ¹é…åº¦
                score = self._calculate_keyword_score(query, material, prompt)
                results.append((material, score, "material"))

            # æœç´¢èŒƒæ–‡
            essays = self.knowledge_base.search_essays(query, top_k=5)
            for essay in essays:
                # è®¡ç®—åŒ¹é…åº¦
                score = self._calculate_keyword_score(query, essay, prompt)
                results.append((essay, score, "essay"))

            return results
        except Exception as e:
            logger.error(f"å…³é”®è¯æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _semantic_retrieval(self, query: str, top_k: int) -> List[Tuple[DocumentChunk, float, str]]:
        """å‘é‡æ£€ç´¢"""
        try:
            # æ‰§è¡Œå‘é‡æœç´¢
            results = self.vector_store.search(query, top_k=top_k)

            # è½¬æ¢ç»“æœæ ¼å¼
            semantic_results = []
            for chunk, score in results:
                content_type = chunk.metadata.get("content_type", "unknown")
                semantic_results.append((chunk, score, content_type))

            return semantic_results
        except Exception as e:
            logger.error(f"è¯­ä¹‰æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _calculate_keyword_score(self, query: str, content: Any, prompt: EssayPrompt) -> float:
        """è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°"""
        try:
            # åŸºç¡€æ–‡æœ¬ç›¸ä¼¼åº¦
            if hasattr(content, 'title') and hasattr(content, 'content'):
                title_score = calculate_similarity(query, content.title)
                content_score = calculate_similarity(query, content.content)
                base_score = title_score * 0.4 + content_score * 0.6
            else:
                base_score = 0.0

            # ç±»å‹åŒ¹é…åŠ åˆ†
            type_bonus = 0.0
            if hasattr(content, 'essay_type') and content.essay_type == prompt.essay_type:
                type_bonus = 0.2

            # éš¾åº¦åŒ¹é…åŠ åˆ†
            difficulty_bonus = 0.0
            if hasattr(content, 'difficulty_level') and content.difficulty_level == prompt.difficulty_level:
                difficulty_bonus = 0.1

            return min(1.0, base_score + type_bonus + difficulty_bonus)
        except Exception as e:
            logger.error(f"è®¡ç®—å…³é”®è¯åˆ†æ•°å¤±è´¥: {e}")
            return 0.0

    def _combine_results(self, keyword_results: List[Tuple], semantic_results: List[Tuple], top_k: int) -> List[Tuple[Any, float, str]]:
        """åˆå¹¶å’Œé‡æ’åºç»“æœ"""
        try:
            combined = {}

            # å¤„ç†å…³é”®è¯ç»“æœ
            for content, score, content_type in keyword_results:
                content_id = getattr(content, 'id', id(content))
                if content_id not in combined:
                    combined[content_id] = {
                        'content': content,
                        'content_type': content_type,
                        'keyword_score': score,
                        'semantic_score': 0.0
                    }
                else:
                    combined[content_id]['keyword_score'] = max(
                        combined[content_id]['keyword_score'], score
                    )

            # å¤„ç†è¯­ä¹‰ç»“æœ
            for chunk, score, content_type in semantic_results:
                # å¯¹äºå‘é‡æ£€ç´¢ç»“æœï¼Œæˆ‘ä»¬éœ€è¦é‡å»ºåŸå§‹å†…å®¹
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨chunkä½œä¸ºå†…å®¹
                content_id = chunk.id
                if content_id not in combined:
                    combined[content_id] = {
                        'content': chunk,
                        'content_type': content_type,
                        'keyword_score': 0.0,
                        'semantic_score': score
                    }
                else:
                    combined[content_id]['semantic_score'] = max(
                        combined[content_id]['semantic_score'], score
                    )

            # è®¡ç®—ç»¼åˆåˆ†æ•°å¹¶æ’åº
            final_results = []
            for content_id, data in combined.items():
                final_score = (
                    data['keyword_score'] * self.keyword_weight +
                    data['semantic_score'] * self.semantic_weight
                )
                final_results.append((data['content'], final_score, data['content_type']))

            # æŒ‰åˆ†æ•°æ’åº
            final_results.sort(key=lambda x: x[1], reverse=True)

            return final_results[:top_k]
        except Exception as e:
            logger.error(f"åˆå¹¶ç»“æœå¤±è´¥: {e}")
            return []

    def _separate_content_types(self, results: List[Tuple[Any, float, str]]) -> Tuple[List[Any], List[Any]]:
        """åˆ†ç¦»ä¸åŒç±»å‹çš„å†…å®¹"""
        materials = []
        essays = []

        for content, score, content_type in results:
            if isinstance(content, WritingMaterial):
                materials.append(content)
            elif isinstance(content, SampleEssay):
                essays.append(content)
            elif isinstance(content, DocumentChunk):
                # æ ¹æ®å…ƒæ•°æ®åˆ¤æ–­ç±»å‹
                chunk_type = content.metadata.get("content_type", "material")
                if chunk_type == "essay":
                    # é‡å»º SampleEssay å¯¹è±¡ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                    essay = SampleEssay(
                        id=content.id,
                        title=content.metadata.get("title", "æœªçŸ¥æ ‡é¢˜"),
                        content=content.content,
                        essay_type=content.metadata.get("essay_type", "narrative"),
                        difficulty_level=content.metadata.get("difficulty_level", "middle")
                    )
                    essays.append(essay)
                else:
                    # é‡å»º WritingMaterial å¯¹è±¡ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                    material = WritingMaterial(
                        id=content.id,
                        title=content.metadata.get("title", "æœªçŸ¥æ ‡é¢˜"),
                        content=content.content,
                        category=content.metadata.get("category", "æœªçŸ¥åˆ†ç±»"),
                        difficulty_level=content.metadata.get("difficulty_level", "middle")
                    )
                    materials.append(material)

        return materials, essays

    def index_knowledge_base(self) -> bool:
        """å°†çŸ¥è¯†åº“å†…å®¹ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“"""
        try:
            chunks = []

            # ç´¢å¼•ç´ æ
            materials = self.knowledge_base.list_materials()
            for material in materials:
                chunk = DocumentChunk(
                    id=f"material_{material.id}",
                    content=f"{material.title}\n\n{material.content}",
                    metadata={
                        "content_type": "material",
                        "title": material.title,
                        "category": material.category,
                        "difficulty_level": material.difficulty_level.value,
                        "keywords": material.keywords
                    },
                    source=f"material_{material.id}",
                    chunk_index=0
                )
                chunks.append(chunk)

            # ç´¢å¼•èŒƒæ–‡
            essays = self.knowledge_base.list_essays()
            for essay in essays:
                chunk = DocumentChunk(
                    id=f"essay_{essay.id}",
                    content=f"{essay.title}\n\n{essay.content}",
                    metadata={
                        "content_type": "essay",
                        "title": essay.title,
                        "essay_type": essay.essay_type.value,
                        "difficulty_level": essay.difficulty_level.value,
                        "score": essay.score
                    },
                    source=f"essay_{essay.id}",
                    chunk_index=0
                )
                chunks.append(chunk)

            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            success = self.vector_store.add_documents(chunks)

            if success:
                logger.info(f"æˆåŠŸç´¢å¼• {len(chunks)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")

            return success
        except Exception as e:
            logger.error(f"ç´¢å¼•çŸ¥è¯†åº“å¤±è´¥: {e}")
            return False
